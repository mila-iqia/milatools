from __future__ import annotations

import asyncio
import contextlib
import errno
import os
import shutil
from logging import getLogger as get_logger
from pathlib import Path, PurePosixPath

import invoke
import pytest
import pytest_asyncio
from paramiko.config import SSHConfig as SshConfigReader

from milatools.cli.init_command import setup_mila_ssh_access
from milatools.cli.utils import SSHConfig
from milatools.utils.local_v2 import run_async
from milatools.utils.remote_v1 import RemoteV1
from milatools.utils.remote_v2 import (
    SSH_CACHE_DIR,
    SSH_CONFIG_FILE,
    RemoteV2,
    get_controlpath_for,
)
from tests.cli.common import in_github_CI, in_self_hosted_github_CI

from ..conftest import mila_username  # noqa

logger = get_logger(__name__)
BACKUP_SSH_DIR = Path.home() / ".ssh_backup"
BACKUP_SSH_CACHE_DIR = Path.home() / ".cache/ssh_backup"

BACKUP_REMOTE_SSH_DIR = ".ssh_backup_remote"

USE_MY_REAL_SSH_DIR = os.environ.get("USE_MY_REAL_SSH_DIR", "0") == "1"
"""Set this to `True` for the tests below to actually use your real SSH directory.

A backup is saved in `BACKUP_SSH_DIR`.
"""

_real_ssh_config = SshConfigReader.from_path(Path.home() / ".ssh" / "config")
_real_mila_username = _real_ssh_config.lookup("mila").get("user")
_real_drac_username = _real_ssh_config.lookup("narval").get("user")


@pytest.fixture
def linux_ssh_config(ssh_config_file: Path):
    """Returns an SSHConfig object containing the config that is generated by `mila
    init`.

    The `mila_username` and `drac_username` fixtures can be indirectly parametrized to
    set the actual `User` value for the mila/drac entries in this config.
    """
    return SSHConfig(ssh_config_file)


@pytest.mark.skipif(
    not USE_MY_REAL_SSH_DIR,
    reason="These tests require potentially modifying the real ~/.ssh directory (with a backup).\n",
)
@pytest.mark.parametrize(mila_username.__name__, [_real_mila_username], indirect=True)
class TestSetupMilaSSHAccess:
    """Test for the portion of `init_command` that sets up SSH access to the Mila
    cluster.

    This is what `init_command` does for the Mila cluster after setting up the SSH config:

    - Checks access to Mila login nodes. If not setup:
        - If there isn't already an SSH keypair in ~/.ssh, runs `ssh-keygen`.
        - Prints the content of the public key in a nice text block that is easy to copy-paste.
        - Instructs the user to send it to it-support@mila.quebec.
    - Checks that everything is setup for compute node node access:
        - Checks that the public key is in ~/.ssh/authorized_keys on the login node.
            If not, copies it explicitly (no ssh-copy-id).
        - Checks that there is an ssh keypair on the login node (with no passphrase)
            and that it is also in ~/.ssh/authorized_keys on the cluster. If not, does it.
        - Checks that the permissions are set correctly on the ~/.ssh directory, keys,
            and ~/.ssh/authorized_keys. If not, corrects them.


    These tests use the real ~/.ssh directory (with a backup in ~/.ssh_backup).
    """

    def real_ssh_config_reader(self, backup_local_ssh_dir: Path) -> SshConfigReader:
        return SshConfigReader.from_path(backup_local_ssh_dir / "config")

    @pytest.mark.asyncio
    async def test_temporarily_disable_shared_connection(
        self,
        backup_local_ssh_dir: Path,
        backup_local_ssh_cache_dir: Path,
        linux_ssh_config: SSHConfig,
    ):
        """Test that we can temporarily disable a shared SSH connection during tests."""
        ssh_cache_dir = Path.home() / ".cache" / "ssh"
        # Make sure that there aren't any active connections to the Mila cluster before
        # the test runs.

        if (await _check_shared_connection_returncode("mila")) != 0:
            pytest.skip(reason="This test needs an existing connection.")

        async with temporarily_disable_shared_ssh_connection(
            "mila", ssh_cache_dir, backup_local_ssh_cache_dir
        ):
            # Inside this block, the shared connection should be disabled.
            assert (await _check_shared_connection_returncode("mila")) == 255

        # Check that we successfully re-enabled the shared connection.
        assert (await _check_shared_connection_returncode("mila")) == 0

    @pytest_asyncio.fixture
    async def no_existing_mila_connection(
        self, backup_local_ssh_dir: Path, backup_local_ssh_cache_dir: Path
    ):
        async with temporarily_disable_shared_ssh_connection(
            "mila", SSH_CACHE_DIR, backup_local_ssh_cache_dir
        ):
            yield

    @pytest.mark.asyncio
    async def test_mila_access_already_setup(
        self,
        backup_local_ssh_dir: Path,
        backup_local_ssh_cache_dir: Path,
        linux_ssh_config: SSHConfig,
        no_existing_mila_connection: None,
    ):
        """Test that if everything is already setup correctly, nothing is changed and
        `setup_mila_ssh_access` just returns True.

        Using the existing SSH directory contents (same keypair) and the SSH config
        that would be generated from `mila init`, we should have access to the Mila
        login nodes, and so `setup_mila_ssh_access` shouldn't change anything and should
        just return True.
        """
        ssh_dir = Path.home() / ".ssh"
        # Make sure that there aren't any active connections to the Mila cluster before
        # the test runs.
        assert (await _check_shared_connection_returncode("mila")) != 0

        files_before = list(ssh_dir.iterdir())

        ssh_config_path = ssh_dir / "config"
        ssh_config_path.write_text(linux_ssh_config.cfg.config())

        assert (
            setup_mila_ssh_access(ssh_config_path.parent, ssh_config=linux_ssh_config)
            is True
        )
        assert list(ssh_dir.iterdir()) == files_before

    @pytest.mark.asyncio
    async def test_missing_compute_node_step(
        self,
        backup_local_ssh_dir: Path,
        backup_local_ssh_cache_dir: Path,
        backup_remote_ssh_dir: Path,
        linux_ssh_config: SSHConfig,
        remote_ssh_dir: PurePosixPath,
        login_node_v2: RemoteV2,
    ):
        """Test that if we have access to the login nodes, but we're missing the step
        where the local public key is added in the ~/.ssh/authorized_keys file, that
        running `mila init` adds it.

        PYTEST_DONT_REWRITE
        """
        # TODO: look into ways to disable assert rewriting for this test
        # https://docs.pytest.org/en/7.1.x/how-to/assert.html#disabling-assert-rewriting
        ssh_dir = Path.home() / ".ssh"

        authorized_public_keys = (
            await login_node_v2.get_output_async(
                "cat ~/.ssh/authorized_keys", hide=True
            )
        ).splitlines()

        local_public_key: Path | str | None = None
        _private_key_path = _real_ssh_config.lookup("mila").get("identityfile")
        if isinstance(_private_key_path, list):
            # weirdly enough, getting the value for identityfile returns a list if it is
            # in the config entry.
            local_public_key = Path(_private_key_path[0]).with_suffix(".pub")
        elif not _private_key_path:
            local_public_key = next(iter((Path.home() / ".ssh").glob("id_*.pub")), None)
        del _private_key_path  # just to be a bit cautious.

        if local_public_key is None or not local_public_key.exists():
            raise RuntimeError(
                f"Couldn't find a SSH public key in {ssh_dir} for the Mila cluster!"
            )

        local_public_key = Path(local_public_key).expanduser()
        local_public_key = local_public_key.read_text().strip()

        # Remove the local public key from the authorized_keys file if it's there.
        # (it might be there multiple times if the authorized keys file was edited manually).
        while local_public_key in authorized_public_keys:
            authorized_public_keys.remove(local_public_key)

        # Re-write the authorized_keys file without this local public key.
        await login_node_v2.run_async(
            "echo '" + "\n".join(authorized_public_keys) + "' > ~/.ssh/authorized_keys",
            hide=True,
            display=False,
        )

        assert (
            local_public_key
            not in (
                await login_node_v2.get_output_async(
                    "cat ~/.ssh/authorized_keys", hide=True
                )
            ).splitlines()
        )

        setup_mila_ssh_access(ssh_dir=ssh_dir, ssh_config=linux_ssh_config)

        authorized_public_keys = (
            await login_node_v2.get_output_async(
                "cat ~/.ssh/authorized_keys", hide=True
            )
        ).splitlines()

        # TODO: Test failure message should ideally not output all the content of authorized_keys!
        # For now, using --assert=plain seems to work, or deleting the variables before asserting.
        # This is very much less than optimal though!
        check = local_public_key in authorized_public_keys
        # assert local_public_key in authorized_public_keys  # BAD! Outputs all authorized_keys on failure.
        del local_public_key
        del authorized_public_keys
        assert check is True
        # assert authorized_public_keys[-1] == local_public_key


async def _check_shared_connection_returncode(hostname: str) -> int:
    _result = await run_async(("ssh", "-O", "check", hostname), hide=True, warn=True)
    return _result.returncode


@contextlib.asynccontextmanager
async def temporarily_disable_shared_ssh_connection(
    hostname: str, ssh_cache_dir: Path, backup_local_ssh_cache_dir: Path
):
    """Temporarily disable a shared SSH connection."""
    ssh_dir = Path.home() / ".ssh"
    ssh_cache_dir = Path.home() / ".cache" / "ssh"
    # Make sure that there aren't any active connections to the Mila cluster before
    # the test runs.
    try:
        control_path = get_controlpath_for(
            hostname, ssh_config_path=ssh_dir / "config", ssh_cache_dir=ssh_cache_dir
        )
    except RuntimeError:
        # host doesn't have a ControlPath/ControlMaster/ControlPersist directive and
        # the ssh cache directory doesn't exist.
        control_path = None

    if (await _check_shared_connection_returncode(hostname)) != 0:
        logger.debug(
            f"There is no shared SSH connection to {hostname} at {control_path}."
        )
        yield control_path
        return

    # There is already an active (shared) connection to the cluster.
    # control_path = (
    #     SshConfigReader.from_path(Path.home() / ".ssh" / "config")
    #     .lookup("mila")
    #     .get("controlpath")
    # )
    assert control_path is not None
    control_path = Path(control_path).expanduser().resolve()
    assert control_path.exists()

    logger.debug(
        f"Temporarily removing shared SSH connection to {hostname} at {control_path}"
    )
    # Make sure that we can disable the shared connection by removing the socket file.
    # (the socket already has a backup in the ssh cache backup dir).
    # backup_of_socket_file = control_path.rename(control_path.with_suffix(".bak"))
    backup_of_socket_file = backup_local_ssh_cache_dir / control_path.name
    assert backup_of_socket_file.exists()

    # "remove" the socket file (there is a backup).
    control_path.unlink()

    yield control_path

    assert backup_of_socket_file.exists()
    # Restore the socket by making `control_path` a hard-link to the backup.
    # (same as `os.link(backup_of_socket_file, control_path)`)
    if control_path.exists():
        # Test must have re-created the socket file! Remove the new one first.
        await run_async(
            ("ssh", "-o", f"ControlPath='{control_path}'", "-O", "exit", hostname),
            hide=False,
        )
        await asyncio.sleep(1)
        assert not control_path.exists()

    backup_of_socket_file.link_to(control_path)


@contextlib.contextmanager
def _backup_dir(directory: Path, backup_directory: Path):
    dir_existed_before = directory.exists()

    def copy_fn(source, dest):
        source = Path(source)
        dest = Path(dest)
        try:
            return shutil.copy2(source, dest)
        except OSError as err:
            if err.errno != errno.ENXIO:
                raise
            # This error (errno 6) happens when trying to copy a socket file.
            # In this case, we make a hard link in the backup dir.

            os.link(source, dest)
            return dest

    if dir_existed_before:
        logger.warning(f"Backing up {directory} to {backup_directory}")
        if backup_directory.exists():
            # shutil.rmtree(backup_directory)
            raise RuntimeError(
                f"The backup directory {backup_directory} to be used as backup for "
                f"{directory} already exists! "
                f"Refusing to remove it to avoid losing backed up files. "
                f"(Consider manually restoring {directory} from {backup_directory})."
            )
        shutil.copytree(directory, backup_directory, copy_function=copy_fn)
    else:
        logger.warning(f"Test might temporarily create files in {directory}.")

    try:
        yield backup_directory
    finally:
        if dir_existed_before:
            logger.warning(f"Restoring {directory} from backup at {backup_directory}")
            if directory.exists():
                shutil.rmtree(directory)

            shutil.copytree(backup_directory, directory, copy_function=copy_fn)
            shutil.rmtree(backup_directory)
        else:
            logger.warning(f"Removing temporarily generated dir {directory}.")
            if directory.exists():
                shutil.rmtree(directory)


# todo: make async and drop support for RemoteV1
@contextlib.contextmanager
def _backup_remote_dir(
    remote: RemoteV2 | RemoteV1,
    directory: PurePosixPath,
    backup_directory: PurePosixPath,
):
    # IDEA: Make the equivalent function, but that backs up a directory on a remote
    # machine.
    def _exists(dir: PurePosixPath) -> bool:
        result = remote.run(f"test -d {dir}", display=True, warn=True, hide=True)
        if isinstance(result, invoke.runners.Result):
            return result.return_code == 0
        return result.returncode == 0

    assert not _exists(PurePosixPath("/does/not/exist"))
    if remote.hostname == "localhost":
        assert _exists(PurePosixPath(Path.cwd()))

    def _rmtree(dir: PurePosixPath):
        return remote.run(f"rm -r {dir}", display=True, hide=False)

    def _copytree(source_dir: PurePosixPath, dest_dir: PurePosixPath):
        remote.run(f"mkdir -p {dest_dir.parent}", display=True)
        return remote.run(f"cp -r {source_dir} {dest_dir}", display=True)

    dir_existed_before = _exists(directory)

    if dir_existed_before:
        logger.warning(
            f"Backing up {directory} to {backup_directory} on the "
            f"{remote.hostname} cluster."
        )
        if _exists(backup_directory):
            # _rmtree(backup_directory)
            raise RuntimeError(
                f"The backup directory {backup_directory} to be used as backup for "
                f"{directory} already exists on the {remote.hostname} cluster! "
                f"Refusing to remove it to avoid losing backed up files. "
                f"(Consider logging in to {remote.hostname} and manually restoring "
                f"{directory} from {backup_directory})."
            )
        _copytree(directory, backup_directory)
    else:
        logger.warning(
            f"Test might temporarily create files in {directory} on the "
            f"{remote.hostname} cluster."
        )
    try:
        yield backup_directory
    except Exception:
        logger.critical(
            f"An error occurred while running the test. Will still attempt to restore "
            f"dir {directory} from backup at {backup_directory} on the "
            f"{remote.hostname} cluster!"
        )
        raise
    finally:
        if dir_existed_before:
            logger.warning(
                f"Restoring {directory} from backup at {backup_directory} on the "
                f"{remote.hostname} cluster."
            )
            if _exists(directory):
                _rmtree(directory)
            _copytree(backup_directory, directory)
            _rmtree(backup_directory)
        else:
            logger.warning(
                f"Removing temporarily generated dir {directory} on the "
                f"{remote.hostname} cluster."
            )
            if _exists(directory):
                _rmtree(directory)


@pytest.fixture
def backup_local_ssh_dir():
    """Creates a backup of the ~/.ssh dir on the local machine to `BACKUP_SSH_DIR`."""
    if not ((in_github_CI and not in_self_hosted_github_CI) or USE_MY_REAL_SSH_DIR):
        pytest.skip(
            reason=(
                "This test requires potentially modifying the ~/.ssh directory (with a backup).\n"
                "To enable these (potentially hasardous) tests, first create a backup of "
                "~/.ssh yourself, and then set `USE_MY_REAL_SSH_DIR=1`."
            )
        )
    ssh_dir = SSH_CONFIG_FILE.parent
    backup_ssh_dir = BACKUP_SSH_DIR
    with _backup_dir(ssh_dir, backup_ssh_dir):
        yield backup_ssh_dir


@pytest.fixture
def backup_local_ssh_cache_dir():
    """Creates a backup of the `SSH_CACHE_DIR` dir on the local machine to
    `BACKUP_SSH_CACHE_DIR`."""
    if not ((in_github_CI and not in_self_hosted_github_CI) or USE_MY_REAL_SSH_DIR):
        pytest.skip(
            reason=(
                "This test requires potentially modifying the ~/.ssh directory (with a backup).\n"
                "To enable these (potentially hasardous) tests, first create a backup of "
                "~/.ssh yourself, and then set `USE_MY_REAL_SSH_DIR=1`."
            )
        )
    ssh_cache_dir = SSH_CACHE_DIR
    backup_ssh_cache_dir = BACKUP_SSH_CACHE_DIR
    with _backup_dir(ssh_cache_dir, backup_ssh_cache_dir):
        yield backup_ssh_cache_dir


# todo: make into an async fixture and drop support for RemoteV1
@pytest.fixture
def remote_ssh_dir(
    login_node: RemoteV2 | RemoteV1,
) -> PurePosixPath:
    """Returns the home directory on the remote cluster."""

    home_on_cluster = login_node.get_output("echo $HOME")
    remote_ssh_dir = PurePosixPath(home_on_cluster) / ".ssh"
    return remote_ssh_dir


@pytest.fixture
def backup_remote_ssh_dir(
    login_node: RemoteV2 | RemoteV1, cluster: str, remote_ssh_dir: PurePosixPath
):
    """Creates a backup of the ~/.ssh directory on the remote cluster."""
    if USE_MY_REAL_SSH_DIR:
        logger.critical(
            f"Running a test that is probably going to modify the REAL '~/.ssh' "
            f"directory on the {cluster} cluster! Make sure you know what you're doing!"
            f"(The directory will be backed up to the {BACKUP_REMOTE_SSH_DIR} folder "
            "and should (hopefully) be restored correctly after the tests run (even if "
            "they fail)."
        )
    else:
        assert in_github_CI and not in_self_hosted_github_CI

    backup_remote_ssh_dir = remote_ssh_dir.parent / BACKUP_REMOTE_SSH_DIR
    with _backup_remote_dir(login_node, remote_ssh_dir, backup_remote_ssh_dir):
        yield backup_remote_ssh_dir
